{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UZ0ou6Dn8SDoRSBGbGO-LoAXuGuxjezq",
      "authorship_tag": "ABX9TyOnOGQS6D3HDPMKnCInQCjp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srikanth1304/rep/blob/main/computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPPy-L9LK9P1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_images_from_folder(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img = os.path.join(folder_path, filename)\n",
        "        # if img is not None:\n",
        "        #     img = cv2.resize(img, (227, 227))  # Resize the images to match the input shape of AlexNet\n",
        "        #     img=np.array(img)\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Define the two folders containing the images and their corresponding labels\n",
        "folder_path_classA = r\"/content/drive/MyDrive/datset/datset/non_pcos\"\n",
        "folder_path_classB = r\"/content/drive/MyDrive/datset/datset/pcos\"\n",
        "\n",
        "# Load images and labels for classA\n",
        "images_classA, labels_classA = load_images_from_folder(folder_path_classA, label=\"non_pcos\")\n",
        "\n",
        "# Load images and labels for classB\n",
        "images_classB, labels_classB = load_images_from_folder(folder_path_classB, label=\"pcos\")\n",
        "\n",
        "\n",
        "# Combine the data from both classes into a single DataFrame\n",
        "data = {\n",
        "    \"image\": images_classA + images_classB,\n",
        "    \"label\": labels_classA + labels_classB\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Shuffle the DataFrame (optional)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Now, the DataFrame 'df' contains the image data and corresponding labels from both folders.\n",
        "# You can use this DataFrame for further processing or training your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels in the DataFrame\n",
        "df[\"encoded_label\"] = label_encoder.fit_transform(df[\"label\"])\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwFEoK2TLgTv",
        "outputId": "146d08c2-7704-4299-9314-e3fedcbf1dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  image     label  \\\n",
            "0     /content/drive/MyDrive/datset/datset/pcos/17im...      pcos   \n",
            "1     /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "2     /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "3     /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "4     /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "...                                                 ...       ...   \n",
            "1042  /content/drive/MyDrive/datset/datset/pcos/1779...      pcos   \n",
            "1043  /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "1044  /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "1045  /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "1046  /content/drive/MyDrive/datset/datset/non_pcos/...  non_pcos   \n",
            "\n",
            "      encoded_label  \n",
            "0                 1  \n",
            "1                 1  \n",
            "2                 1  \n",
            "3                 1  \n",
            "4                 1  \n",
            "...             ...  \n",
            "1042              1  \n",
            "1043              1  \n",
            "1044              1  \n",
            "1045              1  \n",
            "1046              0  \n",
            "\n",
            "[1047 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wmDEvR32Lk3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path, image_width, image_height):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((image_width, image_height))\n",
        "    image_array = np.array(image) / 255.0\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "xYJm8h9dLoOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "X = np.array([load_and_preprocess_image(str(path), 224,224) for path in df['image']])\n",
        "y = to_categorical(df['encoded_label'])"
      ],
      "metadata": {
        "id": "7WVdFU8CLrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "image_width = 224\n",
        "image_height = 224\n",
        "X = np.array([load_and_preprocess_image(str(path), image_width, image_height) for path in df['image']])\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_width, image_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(2, activation='softmax')  # Output layer with 2 units for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a checkpoint callback to save the best model\n",
        "checkpoint_path = \"model_checkpoint.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                      save_best_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=30,\n",
        "          batch_size=32,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[checkpoint_callback])\n",
        "\n",
        "# Load the best model from the checkpoint\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Save the model as HDF5 file\n",
        "model.save(\"dense_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kHLYUwgLt4l",
        "outputId": "3eaab4cf-ec4e-4b79-87e5-f3cd3afcfd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.4370 - accuracy: 0.8219\n",
            "Epoch 1: val_loss improved from inf to 0.27362, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21/21 [==============================] - 4s 101ms/step - loss: 0.4340 - accuracy: 0.8236 - val_loss: 0.2736 - val_accuracy: 0.9167\n",
            "Epoch 2/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.1949 - accuracy: 0.9125\n",
            "Epoch 2: val_loss improved from 0.27362 to 0.20849, saving model to model_checkpoint.h5\n",
            "21/21 [==============================] - 2s 83ms/step - loss: 0.2090 - accuracy: 0.9103 - val_loss: 0.2085 - val_accuracy: 0.9405\n",
            "Epoch 3/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.1701 - accuracy: 0.9219\n",
            "Epoch 3: val_loss improved from 0.20849 to 0.12430, saving model to model_checkpoint.h5\n",
            "21/21 [==============================] - 3s 171ms/step - loss: 0.1653 - accuracy: 0.9238 - val_loss: 0.1243 - val_accuracy: 0.9643\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9432\n",
            "Epoch 4: val_loss did not improve from 0.12430\n",
            "21/21 [==============================] - 1s 62ms/step - loss: 0.1418 - accuracy: 0.9432 - val_loss: 0.1361 - val_accuracy: 0.9345\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9537\n",
            "Epoch 5: val_loss did not improve from 0.12430\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 0.1260 - accuracy: 0.9537 - val_loss: 0.2087 - val_accuracy: 0.9405\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9507\n",
            "Epoch 6: val_loss improved from 0.12430 to 0.12118, saving model to model_checkpoint.h5\n",
            "21/21 [==============================] - 2s 74ms/step - loss: 0.1153 - accuracy: 0.9507 - val_loss: 0.1212 - val_accuracy: 0.9643\n",
            "Epoch 7/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.1085 - accuracy: 0.9625\n",
            "Epoch 7: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 0.1051 - accuracy: 0.9641 - val_loss: 0.2014 - val_accuracy: 0.9345\n",
            "Epoch 8/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.1052 - accuracy: 0.9500\n",
            "Epoch 8: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.1033 - accuracy: 0.9507 - val_loss: 0.1274 - val_accuracy: 0.9345\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9716\n",
            "Epoch 9: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.0748 - accuracy: 0.9716 - val_loss: 0.1537 - val_accuracy: 0.9583\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9851\n",
            "Epoch 10: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 0.0431 - accuracy: 0.9851 - val_loss: 0.1977 - val_accuracy: 0.9345\n",
            "Epoch 11/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.0305 - accuracy: 0.9891\n",
            "Epoch 11: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 0.0331 - accuracy: 0.9865 - val_loss: 0.2399 - val_accuracy: 0.9524\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9895\n",
            "Epoch 12: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 0.1765 - val_accuracy: 0.9524\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9970\n",
            "Epoch 13: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.2550 - val_accuracy: 0.9643\n",
            "Epoch 14/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.0173 - accuracy: 0.9922\n",
            "Epoch 14: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 0.0188 - accuracy: 0.9910 - val_loss: 0.2251 - val_accuracy: 0.9464\n",
            "Epoch 15/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 15: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2912 - val_accuracy: 0.9464\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 6.4101e-04 - accuracy: 1.0000\n",
            "Epoch 16: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 6.4101e-04 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 0.9464\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.8572e-04 - accuracy: 1.0000\n",
            "Epoch 17: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 1.8572e-04 - accuracy: 1.0000 - val_loss: 0.3661 - val_accuracy: 0.9524\n",
            "Epoch 18/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 7.4752e-05 - accuracy: 1.0000\n",
            "Epoch 18: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 7.1946e-05 - accuracy: 1.0000 - val_loss: 0.3675 - val_accuracy: 0.9524\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 4.5160e-05 - accuracy: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 4.5160e-05 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 0.9524\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 3.4988e-05 - accuracy: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 64ms/step - loss: 3.4988e-05 - accuracy: 1.0000 - val_loss: 0.3776 - val_accuracy: 0.9524\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.9836e-05 - accuracy: 1.0000\n",
            "Epoch 21: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 65ms/step - loss: 2.9836e-05 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 0.9524\n",
            "Epoch 22/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 2.6514e-05 - accuracy: 1.0000\n",
            "Epoch 22: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 2.5678e-05 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 0.9524\n",
            "Epoch 23/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 2.1871e-05 - accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 2.2355e-05 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9524\n",
            "Epoch 24/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 2.0749e-05 - accuracy: 1.0000\n",
            "Epoch 24: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 61ms/step - loss: 1.9960e-05 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 0.9524\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.7870e-05 - accuracy: 1.0000\n",
            "Epoch 25: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 1.7870e-05 - accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9524\n",
            "Epoch 26/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.6182e-05 - accuracy: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 59ms/step - loss: 1.6210e-05 - accuracy: 1.0000 - val_loss: 0.3943 - val_accuracy: 0.9524\n",
            "Epoch 27/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.4960e-05 - accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 58ms/step - loss: 1.4715e-05 - accuracy: 1.0000 - val_loss: 0.3961 - val_accuracy: 0.9524\n",
            "Epoch 28/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.4098e-05 - accuracy: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 58ms/step - loss: 1.3521e-05 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9524\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2503e-05 - accuracy: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 60ms/step - loss: 1.2503e-05 - accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 0.9524\n",
            "Epoch 30/30\n",
            "20/21 [===========================>..] - ETA: 0s - loss: 1.2052e-05 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.12118\n",
            "21/21 [==============================] - 1s 63ms/step - loss: 1.1550e-05 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.9524\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.1409 - accuracy: 0.9429\n",
            "Test Loss: 0.14094384014606476\n",
            "Test Accuracy: 0.9428571462631226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlXFrG6VTpd4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model from the H5 file\n",
        "loaded_model = load_model('/content/dense_model.h5')\n"
      ],
      "metadata": {
        "id": "vllcb8OXL4U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Assuming you have a pre-trained feature extractor model (e.g., a CNN)\n",
        "pretrained_feature_extractor = loaded_model.layers[-3]\n",
        "\n",
        "# Get the output shape of the feature extraction layer\n",
        "feature_shape = pretrained_feature_extractor.output_shape[1:]\n",
        "\n",
        "base_model = models.Model(inputs=loaded_model.input, outputs=loaded_model.layers[-3].output)\n",
        "# Print the shape for debugging\n",
        "print(\"Feature Shape:\", feature_shape)\n",
        "\n",
        "# Define your transformer model\n",
        "def transformer_model(input_shape, num_transformer_layers, output_dim):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Add a \"sequence_length\" dimension to the input\n",
        "    sequence = layers.Reshape((1, input_shape[0]))(inputs)\n",
        "\n",
        "    # Transformer layers using MultiHeadAttention\n",
        "    sequence = MultiHeadAttention(num_heads=5, key_dim=64)(sequence, sequence)\n",
        "    sequence = layers.LayerNormalization(epsilon=1e-6)(sequence)\n",
        "    sequence = layers.Dropout(0.1)(sequence)\n",
        "\n",
        "    # Reshape the sequence to maintain 3D structure\n",
        "    sequence = layers.Reshape((-1, feature_shape[0]))(sequence)\n",
        "\n",
        "    # Output layer for your specific task\n",
        "    outputs = layers.Dense(output_dim, activation='softmax')(sequence)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Define your task-specific output dimension (e.g., number of classes)\n",
        "output_dim = 2\n",
        "num_tlayers = 2\n",
        "\n",
        "# Create the transformer model\n",
        "transformer = transformer_model(input_shape=feature_shape, num_transformer_layers=num_tlayers, output_dim=output_dim)\n",
        "\n",
        "# Print the summary of the model for debugging\n",
        "transformer.summary()\n",
        "\n",
        "# Combine the pre-trained feature extractor with the transformer\n",
        "combined_model = models.Sequential([\n",
        "    base_model,\n",
        "    transformer,\n",
        "    layers.Flatten()  # Add Flatten layer to reshape the output\n",
        "])\n",
        "\n",
        "# Compile the model (adjust loss and metrics based on your task)\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the combined model\n",
        "combined_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3x_Da5NL7p7",
        "outputId": "2caa6cd8-46ec-449b-935f-70f6a2b5b0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Shape: (18432,)\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 18432)]              0         []                            \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 1, 18432)             0         ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 1, 18432)             2361235   ['reshape_2[0][0]',           \n",
            " ltiHeadAttention)                                        2          'reshape_2[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 1, 18432)             36864     ['multi_head_attention_1[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 1, 18432)             0         ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 1, 18432)             0         ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1, 2)                 36866     ['reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23686082 (90.36 MB)\n",
            "Trainable params: 23686082 (90.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_3 (Functional)        (None, 18432)             240832    \n",
            "                                                                 \n",
            " model_4 (Functional)        (None, 1, 2)              23686082  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23926914 (91.27 MB)\n",
            "Trainable params: 23926914 (91.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the combined model on your task-specific data\n",
        "combined_model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF6m6b9zL-zW",
        "outputId": "445cb6fc-990a-47a8-e1bd-2352a58f536d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "27/27 [==============================] - 6s 84ms/step - loss: 0.4670 - accuracy: 0.8961 - val_loss: 0.2272 - val_accuracy: 0.9286\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 0.2040 - accuracy: 0.9379 - val_loss: 0.2370 - val_accuracy: 0.9381\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.1458 - accuracy: 0.9510 - val_loss: 0.2435 - val_accuracy: 0.9286\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.1459 - accuracy: 0.9606 - val_loss: 0.2450 - val_accuracy: 0.9524\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0851 - accuracy: 0.9713 - val_loss: 0.1907 - val_accuracy: 0.9524\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 2s 77ms/step - loss: 0.0675 - accuracy: 0.9785 - val_loss: 0.1906 - val_accuracy: 0.9571\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0585 - accuracy: 0.9773 - val_loss: 0.3008 - val_accuracy: 0.9429\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0587 - accuracy: 0.9761 - val_loss: 0.2717 - val_accuracy: 0.9476\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 0.0578 - accuracy: 0.9809 - val_loss: 0.4264 - val_accuracy: 0.9381\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.1132 - accuracy: 0.9665 - val_loss: 0.3365 - val_accuracy: 0.9476\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0694 - accuracy: 0.9773 - val_loss: 0.2572 - val_accuracy: 0.9476\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0231 - accuracy: 0.9952 - val_loss: 0.4038 - val_accuracy: 0.9429\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 0.0247 - accuracy: 0.9904 - val_loss: 0.3025 - val_accuracy: 0.9476\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.3336 - val_accuracy: 0.9476\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.3427 - val_accuracy: 0.9571\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.3736 - val_accuracy: 0.9476\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.3804 - val_accuracy: 0.9429\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.9524\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4587 - val_accuracy: 0.9381\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9286\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7976a46b0250>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the combined model on your task-specific data\n",
        "combined_model.fit(X,y, epochs=30, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQNUyffjMCNR",
        "outputId": "a2cd7a71-bc17-45b8-e750-9eb546abf03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "27/27 [==============================] - 2s 70ms/step - loss: 3.1956e-04 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.9524\n",
            "Epoch 2/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 3.1033e-04 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.9571\n",
            "Epoch 3/30\n",
            "27/27 [==============================] - 2s 66ms/step - loss: 2.3028e-04 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.9571\n",
            "Epoch 4/30\n",
            "27/27 [==============================] - 2s 69ms/step - loss: 2.4870e-04 - accuracy: 1.0000 - val_loss: 0.5347 - val_accuracy: 0.9524\n",
            "Epoch 5/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 2.4608e-04 - accuracy: 1.0000 - val_loss: 0.5381 - val_accuracy: 0.9524\n",
            "Epoch 6/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.9979e-04 - accuracy: 1.0000 - val_loss: 0.5424 - val_accuracy: 0.9571\n",
            "Epoch 7/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 1.9042e-04 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.9524\n",
            "Epoch 8/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 1.9230e-04 - accuracy: 1.0000 - val_loss: 0.5428 - val_accuracy: 0.9524\n",
            "Epoch 9/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.8449e-04 - accuracy: 1.0000 - val_loss: 0.5450 - val_accuracy: 0.9571\n",
            "Epoch 10/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.7987e-04 - accuracy: 1.0000 - val_loss: 0.5454 - val_accuracy: 0.9571\n",
            "Epoch 11/30\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 1.5344e-04 - accuracy: 1.0000 - val_loss: 0.5459 - val_accuracy: 0.9524\n",
            "Epoch 12/30\n",
            "27/27 [==============================] - 2s 67ms/step - loss: 1.4435e-04 - accuracy: 1.0000 - val_loss: 0.5484 - val_accuracy: 0.9571\n",
            "Epoch 13/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.5374e-04 - accuracy: 1.0000 - val_loss: 0.5495 - val_accuracy: 0.9571\n",
            "Epoch 14/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.2920e-04 - accuracy: 1.0000 - val_loss: 0.5480 - val_accuracy: 0.9524\n",
            "Epoch 15/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.4033e-04 - accuracy: 1.0000 - val_loss: 0.5512 - val_accuracy: 0.9571\n",
            "Epoch 16/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.0782e-04 - accuracy: 1.0000 - val_loss: 0.5532 - val_accuracy: 0.9571\n",
            "Epoch 17/30\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 1.0774e-04 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.9571\n",
            "Epoch 18/30\n",
            "27/27 [==============================] - 2s 74ms/step - loss: 1.2540e-04 - accuracy: 1.0000 - val_loss: 0.5552 - val_accuracy: 0.9571\n",
            "Epoch 19/30\n",
            "27/27 [==============================] - 2s 68ms/step - loss: 1.1778e-04 - accuracy: 1.0000 - val_loss: 0.5548 - val_accuracy: 0.9571\n",
            "Epoch 20/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.0721e-04 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.9524\n",
            "Epoch 21/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.0735e-04 - accuracy: 1.0000 - val_loss: 0.5552 - val_accuracy: 0.9524\n",
            "Epoch 22/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 9.8434e-05 - accuracy: 1.0000 - val_loss: 0.5594 - val_accuracy: 0.9571\n",
            "Epoch 23/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 1.0222e-04 - accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9524\n",
            "Epoch 24/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 8.3512e-05 - accuracy: 1.0000 - val_loss: 0.5615 - val_accuracy: 0.9571\n",
            "Epoch 25/30\n",
            "27/27 [==============================] - 2s 72ms/step - loss: 8.5311e-05 - accuracy: 1.0000 - val_loss: 0.5612 - val_accuracy: 0.9571\n",
            "Epoch 26/30\n",
            "27/27 [==============================] - 2s 75ms/step - loss: 7.4662e-05 - accuracy: 1.0000 - val_loss: 0.5617 - val_accuracy: 0.9571\n",
            "Epoch 27/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 7.6541e-05 - accuracy: 1.0000 - val_loss: 0.5647 - val_accuracy: 0.9571\n",
            "Epoch 28/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 6.7770e-05 - accuracy: 1.0000 - val_loss: 0.5633 - val_accuracy: 0.9571\n",
            "Epoch 29/30\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 8.3888e-05 - accuracy: 1.0000 - val_loss: 0.5686 - val_accuracy: 0.9571\n",
            "Epoch 30/30\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 7.2736e-05 - accuracy: 1.0000 - val_loss: 0.5670 - val_accuracy: 0.9571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79767cd1ac80>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if TensorFlow is using GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Print GPU details\n",
        "print(\"GPU Details:\")\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    print(gpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9IdCgMEME6S",
        "outputId": "5ec1f643-1313-464a-9ccc-2f15667589ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "GPU Details:\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_model.save('VIT.plt')"
      ],
      "metadata": {
        "id": "GNJ-ccDvMHi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_images_from_folder(folder_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img = os.path.join(folder_path, filename)\n",
        "        # if img is not None:\n",
        "        #     img = cv2.resize(img, (227, 227))  # Resize the images to match the input shape of AlexNet\n",
        "        #     img=np.array(img)\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Define the two folders containing the images and their corresponding labels\n",
        "folder_path_classA = r\"/content/drive/MyDrive/datset/datset/non_pcos\"\n",
        "folder_path_classB = r\"/content/drive/MyDrive/datset/datset/pcos\"\n",
        "\n",
        "# Load images and labels for classA\n",
        "images_classA, labels_classA = load_images_from_folder(folder_path_classA, label=\"pcos\")\n",
        "\n",
        "# Load images and labels for classB\n",
        "images_classB, labels_classB = load_images_from_folder(folder_path_classB, label=\"non_pcos\")\n",
        "\n",
        "\n",
        "# Combine the data from both classes into a single DataFrame\n",
        "data = {\n",
        "    \"image\": images_classA + images_classB,\n",
        "    \"label\": labels_classA + labels_classB\n",
        "}\n",
        "\n",
        "dft= pd.DataFrame(data)\n",
        "\n",
        "# Shuffle the DataFrame (optional)\n",
        "dft = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Now, the DataFrame 'df' contains the image data and corresponding labels from both folders.\n",
        "# You can use this DataFrame for further processing or training your model.\n"
      ],
      "metadata": {
        "id": "qrjkxEzWMKL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels in the DataFrame\n",
        "dft[\"encoded_label\"] = label_encoder.fit_transform(dft[\"label\"])\n",
        "\n",
        "print(dft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PNoynRwM6sJ",
        "outputId": "fc812b0d-4b33-415e-a001-b18df8b03cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  image     label  \\\n",
            "0     /content/drive/MyDrive/datset/datset/pcos/553i...      pcos   \n",
            "1     /content/drive/MyDrive/datset/datset/non_pcos/...  non_pcos   \n",
            "2     /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "3     /content/drive/MyDrive/datset/datset/non_pcos/...  non_pcos   \n",
            "4     /content/drive/MyDrive/datset/datset/pcos/99im...      pcos   \n",
            "...                                                 ...       ...   \n",
            "1042  /content/drive/MyDrive/datset/datset/pcos/24im...      pcos   \n",
            "1043  /content/drive/MyDrive/datset/datset/pcos/25im...      pcos   \n",
            "1044  /content/drive/MyDrive/datset/datset/pcos/87im...      pcos   \n",
            "1045  /content/drive/MyDrive/datset/datset/non_pcos/...  non_pcos   \n",
            "1046  /content/drive/MyDrive/datset/datset/pcos/imag...      pcos   \n",
            "\n",
            "      encoded_label  \n",
            "0                 1  \n",
            "1                 0  \n",
            "2                 1  \n",
            "3                 0  \n",
            "4                 1  \n",
            "...             ...  \n",
            "1042              1  \n",
            "1043              1  \n",
            "1044              1  \n",
            "1045              0  \n",
            "1046              1  \n",
            "\n",
            "[1047 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tYylG6wfM9vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(image_path, image_width, image_height):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((image_width, image_height))\n",
        "    image_array = np.array(image) / 255.0\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "qingzoOCM9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Xt = np.array([load_and_preprocess_image(str(path), 224,224) for path in dft['image']])\n",
        "yt = to_categorical(dft['encoded_label'])"
      ],
      "metadata": {
        "id": "S64RJpHoM9pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the model from the H5 file\n",
        "loaded_model = load_model('VIT.plt')\n",
        "\n",
        "# Get the output of the last convolutional layer\n",
        "last_conv_layer_output = loaded_model.layers[-1].output  # Assuming the last convolutional layer is the fourth from the end\n",
        "\n",
        "# Define a new model that outputs the features from the last convolutional layer\n",
        "feature_extraction_model = tf.keras.Model(inputs=loaded_model.input, outputs=last_conv_layer_output)\n",
        "\n",
        "# Use this model to extract features from your data\n",
        "features = feature_extraction_model.predict(Xt)\n",
        "\n",
        "# 'features' now contains the extracted features from the last convolutional layer\n",
        "# You can use these features for further analysis or visualization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qi3npCsM9mI",
        "outputId": "c4f9305f-7bf3-4a60-b630-c0bce3c7acfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 1s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7e70PBLM9ej",
        "outputId": "58311afa-83b6-43ea-e6f1-c81c5d2dac5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.6729615e-06 9.9999130e-01]\n",
            " [1.0000000e+00 1.9862714e-17]\n",
            " [3.5304861e-06 9.9999642e-01]\n",
            " ...\n",
            " [1.0462336e-04 9.9989533e-01]\n",
            " [1.0000000e+00 1.0771153e-17]\n",
            " [5.4925398e-10 1.0000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the model from the H5 file\n",
        "loaded_model = load_model('VIT.plt')\n",
        "\n",
        "# Assuming Xt contains the input images with shape (None, 224, 224, 3)\n",
        "# where None is the batch size and 224x224x3 is the image dimensions and channels\n",
        "# Ensure that Xt has the correct shape\n",
        "print(\"Input data shape:\", Xt.shape)\n",
        "\n",
        "# Use this model to extract features from your data\n",
        "features = loaded_model.predict(Xt)\n",
        "\n",
        "# 'features' now contains the extracted features\n",
        "# You can use these features for further analysis or visualization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6wSPUWBNLPL",
        "outputId": "7a7a27dc-69b3-4558-f6a0-819dd4904093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data shape: (1047, 224, 224, 3)\n",
            "33/33 [==============================] - 1s 26ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model from the H5 file\n",
        "model = load_model('VIT.plt')\n",
        "\n",
        "# Now, you can use the loaded_model to make predictions on new data\n",
        "# For example:\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have new data stored in the 'new_data' variable\n",
        "predictions = model.predict(Xt)\n",
        "\n",
        "# 'predictions' will contain the model's output for the new data\n",
        "# You can now use 'predictions' for further processing or analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVQ4rniDNLLw",
        "outputId": "f6cf1d33-ccd5-43d7-fd5e-f8134603933c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 1s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresholded_predictions = np.zeros_like(predictions)\n",
        "thresholded_predictions[np.arange(len(predictions)), predictions.argmax(axis=1)] = 1\n",
        "\n",
        "print(thresholded_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyq8OYfLNLHy",
        "outputId": "86213f4f-b279-43b7-d509-93cb85eabce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Create a classification report\n",
        "report = classification_report(yt,thresholded_predictions)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LbWCeExNLAj",
        "outputId": "44f86e12-95ee-46da-edbe-43bdf7e5b484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97       144\n",
            "           1       0.99      1.00      1.00       903\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      1047\n",
            "   macro avg       0.99      0.97      0.98      1047\n",
            "weighted avg       0.99      0.99      0.99      1047\n",
            " samples avg       0.99      0.99      0.99      1047\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "# Create a classification report\n",
        "report1 = multilabel_confusion_matrix(yt, thresholded_predictions)\n",
        "\n",
        "# Print the classification report\n",
        "print(report1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqrO2tgjM9Qi",
        "outputId": "27436e5a-c4ce-4859-b984-75b253e60b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[901   2]\n",
            "  [  7 137]]\n",
            "\n",
            " [[137   7]\n",
            "  [  2 901]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class-wise accuracy\n",
        "class_accuracy = np.array([np.diag(conf_matrix) / np.sum(conf_matrix, axis=1) for conf_matrix in report1])\n",
        "\n",
        "# Convert to percentage\n",
        "class_accuracy_percentage = class_accuracy * 100\n",
        "\n",
        "# Display class-wise accuracy\n",
        "for i, acc in enumerate(class_accuracy_percentage):\n",
        "    print(f\"Class {i}: {acc[1]:.2f}%\")  # Assuming the positive class is in the second row (index 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLkJ9Ci7Nctf",
        "outputId": "71d5f57d-7e0a-4173-9aac-7d9b76727c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 95.14%\n",
            "Class 1: 99.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract true positives (TP) from the confusion matrix\n",
        "TP = report1[:, 1, 1]  # Assuming positive class is 1, modify accordingly\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = TP.sum() / len(yt)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHaefB5NgaT",
        "outputId": "d529424c-a62f-41d0-a872-64c0f82a07a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0H3yYaLCNnCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cnuhRdvCPDF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}